"""
RAG Engine - Gestion de l'indexation et des requÃªtes
Utilise Llama-Index pour crÃ©er et interroger un index vectoriel
"""
import os
from typing import List, Dict, Optional
from pathlib import Path

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, PromptTemplate, StorageContext, load_index_from_storage
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

# Essayer d'importer OpenAI, sinon utiliser Ollama
try:
    from llama_index.llms.openai import OpenAI
    from llama_index.embeddings.openai import OpenAIEmbedding
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Importer Ollama comme alternative
try:
    from llama_index.llms.ollama import Ollama
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False


class RAGEngine:
    """Moteur RAG pour SchoolReg"""
    
    def __init__(
        self,
        data_dir: str = "./data",
        storage_dir: str = "./storage",
        openai_api_key: Optional[str] = None,
        model: str = "gpt-4o-mini",
        temperature: float = 0.1,
        similarity_top_k: int = 5
    ):
        """
        Initialise le moteur RAG
        
        Args:
            data_dir: RÃ©pertoire contenant les documents sources
            storage_dir: RÃ©pertoire pour persister l'index
            openai_api_key: ClÃ© API OpenAI
            model: ModÃ¨le OpenAI Ã  utiliser
            temperature: TempÃ©rature pour la gÃ©nÃ©ration
            similarity_top_k: Nombre de passages Ã  rÃ©cupÃ©rer
        """
        self.data_dir = Path(data_dir)
        self.storage_dir = Path(storage_dir)
        self.model = model
        self.temperature = temperature
        self.similarity_top_k = similarity_top_k
        
        # Configuration OpenAI avec fallback automatique vers Ollama
        self.using_openai = False
        self.using_ollama = False
        self.model_info = "unknown"
        
        if openai_api_key and OPENAI_AVAILABLE:
            os.environ["OPENAI_API_KEY"] = openai_api_key
            
            # Essayer d'utiliser OpenAI pour le LLM UNIQUEMENT
            try:
                print("ðŸ”„ Configuration d'OpenAI LLM (rÃ©ponses)...")
                Settings.llm = OpenAI(
                    model=self.model,
                    temperature=self.temperature,
                    max_tokens=2000
                )
                # TOUJOURS utiliser HuggingFace pour les embeddings (gratuit, local, pas de quota)
                print("ðŸ”„ Configuration HuggingFace embeddings (recherche)...")
                Settings.embed_model = HuggingFaceEmbedding(
                    model_name="BAAI/bge-small-en-v1.5"
                )
                self.using_openai = True
                self.model_info = f"OpenAI {self.model} + HuggingFace embeddings"
                print(f"âœ… Mode hybride configurÃ©: {self.model_info}")
                print("ðŸ’¡ LLM: OpenAI (rÃ©ponses) | Embeddings: HuggingFace (recherche)")
            except Exception as e:
                print(f"âš ï¸ OpenAI non disponible: {e}")
                print("ðŸ”„ Basculement automatique vers Ollama...")
                self.using_openai = False
        
        # Si OpenAI Ã©choue ou n'est pas disponible, utiliser Ollama
        if not self.using_openai:
            if OLLAMA_AVAILABLE:
                print("ðŸ”„ Configuration d'Ollama (local & gratuit)...")
                Settings.llm = Ollama(
                    model="llama3.2",
                    temperature=self.temperature,
                    request_timeout=120.0
                )
                Settings.embed_model = HuggingFaceEmbedding(
                    model_name="BAAI/bge-small-en-v1.5"
                )
                self.using_ollama = True
                self.model_info = "Ollama llama3.2 (local)"
                print(f"âœ… Ollama configurÃ©: {self.model_info}")
                print("ðŸ’¡ Note: Assurez-vous qu'Ollama est lancÃ© (ollama serve)")
            else:
                raise RuntimeError(
                    "Ni OpenAI ni Ollama ne sont disponibles. "
                    "Installez ollama ou configurez une clÃ© OpenAI valide."
                )
        
        # Prompt personnalisÃ© en franÃ§ais - Direct et efficace
        self.text_qa_template_str = (
            "Tu es un assistant virtuel pour SchoolReg, une plateforme de gestion scolaire. "
            "Tu dois Ãªtre professionnel, prÃ©cis et utile.\n\n"
            
            "DOCUMENTATION DISPONIBLE:\n"
            "------------------------------\n"
            "{context_str}\n"
            "------------------------------\n\n"
            
            "RÃˆGLES DE RÃ‰PONSE:\n"
            "1. UNIQUEMENT si la question est une salutation initiale (bonjour, salut), rÃ©ponds chaleureusement et prÃ©sente-toi BRIÃˆVEMENT.\n"
            "2. Pour TOUTES les autres questions, va DIRECTEMENT au contenu de la rÃ©ponse SANS salutation prÃ©liminaire.\n"
            "3. Utilise la documentation ci-dessus comme source principale d'information.\n"
            "4. Si l'info n'est pas dans la documentation, explique clairement ce que tu peux faire pour aider.\n"
            "5. Sois concis, clair et direct. Pas de formules de politesse rÃ©pÃ©titives.\n"
            "6. Utilise un Ã©moji occasionnel (1 max par rÃ©ponse) pour Ãªtre accueillant.\n"
            "7. RÃ©ponds en franÃ§ais.\n\n"
            
            "Question: {query_str}\n\n"
            "RÃ©ponse directe:"
        )
        self.text_qa_template = PromptTemplate(self.text_qa_template_str)
        
        # Charger ou crÃ©er l'index
        self.index = None
        self.query_engine = None
        self._load_or_create_index()
    
    def _load_or_create_index(self):
        """Charge l'index existant ou en crÃ©e un nouveau"""
        try:
            if self.storage_dir.exists() and any(self.storage_dir.iterdir()):
                print(f"ðŸ“‚ Chargement de l'index depuis {self.storage_dir}")
                storage_context = StorageContext.from_defaults(persist_dir=str(self.storage_dir))
                self.index = load_index_from_storage(storage_context)
                print("âœ… Index chargÃ© avec succÃ¨s")
            else:
                print(f"ðŸ”¨ CrÃ©ation d'un nouvel index depuis {self.data_dir}")
                self._create_index()
        except Exception as e:
            print(f"âš ï¸ Erreur lors du chargement de l'index: {e}")
            print("ðŸ”¨ Tentative de crÃ©ation d'un nouvel index...")
            self._create_index()
        
        # CrÃ©er le query engine
        if self.index:
            self.query_engine = self.index.as_query_engine(
                text_qa_template=self.text_qa_template,
                similarity_top_k=self.similarity_top_k
            )
    
    def _create_index(self):
        """CrÃ©e un nouvel index Ã  partir des documents"""
        if not self.data_dir.exists():
            raise FileNotFoundError(f"Le dossier {self.data_dir} n'existe pas")
        
        # VÃ©rifier qu'il y a des documents
        files = list(self.data_dir.glob("*"))
        if not files:
            raise ValueError(f"Aucun document trouvÃ© dans {self.data_dir}")
        
        print(f"ðŸ“„ Lecture de {len(files)} fichier(s)...")
        
        # Charger les documents
        documents = SimpleDirectoryReader(
            input_dir=str(self.data_dir),
            recursive=True
        ).load_data()
        
        print(f"ðŸ“Š {len(documents)} document(s) chargÃ©(s)")
        
        # CrÃ©er l'index avec HuggingFace embeddings (toujours)
        print("ðŸ”¨ CrÃ©ation de l'index vectoriel...")
        self.index = VectorStoreIndex.from_documents(
            documents,
            show_progress=True
        )
        print("âœ… Index crÃ©Ã© avec succÃ¨s")
        
        # Sauvegarder l'index
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self.index.storage_context.persist(persist_dir=str(self.storage_dir))
        print(f"ðŸ’¾ Index sauvegardÃ© dans {self.storage_dir}")
    
    def query(self, question: str) -> Dict:
        """
        Interroge l'index et gÃ©nÃ¨re une rÃ©ponse
        
        Args:
            question: Question de l'utilisateur
            
        Returns:
            Dict contenant la rÃ©ponse et les sources
        """
        if not self.query_engine:
            raise RuntimeError("Le query engine n'est pas initialisÃ©")
        
        try:
            # ExÃ©cuter la requÃªte
            response = self.query_engine.query(question)
            
            # Extraire les sources
            sources = []
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes:
                    metadata = node.node.metadata if hasattr(node.node, 'metadata') else {}
                    sources.append({
                        "file_name": metadata.get('file_name', 'Unknown'),
                        "page": metadata.get('page_label'),
                        "score": float(node.score) if hasattr(node, 'score') else None,
                        "text_preview": node.text[:200] + "..." if len(node.text) > 200 else node.text
                    })
            
            return {
                "success": True,
                "question": question,
                "answer": str(response),
                "sources": sources,
                "model_used": self.model_info,
                "using_openai": self.using_openai,
                "using_ollama": self.using_ollama
            }
        
        except Exception as e:
            return {
                "success": False,
                "question": question,
                "error": str(e),
                "model_used": self.model_info
            }
    
    def search(self, question: str) -> Dict:
        """
        Recherche les passages pertinents sans gÃ©nÃ©ration
        
        Args:
            question: Question de l'utilisateur
            
        Returns:
            Dict contenant les passages pertinents
        """
        if not self.index:
            raise RuntimeError("L'index n'est pas initialisÃ©")
        
        try:
            # CrÃ©er un retriever
            retriever = self.index.as_retriever(
                similarity_top_k=self.similarity_top_k
            )
            
            # RÃ©cupÃ©rer les nÅ“uds pertinents
            nodes = retriever.retrieve(question)
            
            # Formater les rÃ©sultats
            results = []
            for node in nodes:
                metadata = node.node.metadata if hasattr(node.node, 'metadata') else {}
                results.append({
                    "file_name": metadata.get('file_name', 'Unknown'),
                    "page": metadata.get('page_label'),
                    "score": float(node.score) if hasattr(node, 'score') else None,
                    "text": node.text
                })
            
            return {
                "success": True,
                "question": question,
                "results": results,
                "count": len(results)
            }
        
        except Exception as e:
            # Fallback runtime: gÃ©rer erreurs embeddings OpenAI et LLM OpenAI
            print(f"âš ï¸ Erreur durant la requÃªte: {e}")
            err = str(e).lower()
            try_again = False
            # 1) ProblÃ¨me embeddings OpenAI (403 / model_not_found)
            if self.using_openai and ("model_not_found" in err or "does not have access to model" in err or "embedding" in err):
                try:
                    print("ðŸ”„ Basculement embeddings âžœ HuggingFace et reconstruction index...")
                    from llama_index.embeddings.huggingface import HuggingFaceEmbedding as _HF
                    Settings.embed_model = _HF(model_name="BAAI/bge-small-en-v1.5")
                    self.model_info = (self.model_info + " + HF embeddings").strip()
                    # RecrÃ©er l'index et le query engine
                    self.refresh_index()
                    try_again = True
                except Exception as e_emb:
                    print(f"âŒ Ã‰chec fallback embeddings: {e_emb}")
            # 2) ProblÃ¨me LLM OpenAI (quota / rate limit / indisponible)
            if (self.using_openai and ("openai" in err or "quota" in err or "rate limit" in err)) and OLLAMA_AVAILABLE:
                print("ðŸ”„ Basculement LLM âžœ Ollama et nouvelle tentative...")
                from llama_index.llms.ollama import Ollama as _O
                Settings.llm = _O(model="llama3.2", temperature=self.temperature, request_timeout=120.0)
                self.using_openai = False
                self.using_ollama = True
                self.model_info = "Ollama llama3.2 (fallback)"
                try_again = True
            if try_again:
                try:
                    response = self.query_engine.query(question)
                    sources = []
                    if hasattr(response, 'source_nodes'):
                        for node in response.source_nodes:
                            metadata = node.node.metadata if hasattr(node.node, 'metadata') else {}
                            sources.append({
                                "file_name": metadata.get('file_name', 'Unknown'),
                                "page": metadata.get('page_label'),
                                "score": float(node.score) if hasattr(node, 'score') else None,
                                "text_preview": node.text[:200] + "..." if len(node.text) > 200 else node.text
                            })
                    return {
                        "success": True,
                        "question": question,
                        "answer": str(response),
                        "sources": sources,
                        "model_used": self.model_info,
                        "using_openai": self.using_openai,
                        "using_ollama": self.using_ollama
                    }
                except Exception as e2:
                    print(f"âŒ Nouvelle tentative Ã©chouÃ©e: {e2}")
                    return {
                        "success": False,
                        "question": question,
                        "error": str(e2),
                        "model_used": self.model_info
                    }
            return {
                "success": False,
                "question": question,
                "error": str(e),
                "model_used": self.model_info
            }
    
    def refresh_index(self):
        """Reconstruit l'index Ã  partir des documents"""
        print("ðŸ”„ Reconstruction de l'index...")
        
        # Supprimer l'ancien index
        if self.storage_dir.exists():
            import shutil
            shutil.rmtree(self.storage_dir)
        
        # RecrÃ©er l'index
        self._create_index()
        
        # RecrÃ©er le query engine
        if self.index:
            self.query_engine = self.index.as_query_engine(
                text_qa_template=self.text_qa_template,
                similarity_top_k=self.similarity_top_k
            )
        
        print("âœ… Index reconstruit avec succÃ¨s")
    
    def get_stats(self) -> Dict:
        """Retourne des statistiques sur l'index"""
        try:
            doc_count = len(list(self.data_dir.glob("*"))) if self.data_dir.exists() else 0
            index_exists = self.storage_dir.exists() and any(self.storage_dir.iterdir())
            
            return {
                "success": True,
                "data_dir": str(self.data_dir),
                "storage_dir": str(self.storage_dir),
                "documents_count": doc_count,
                "index_exists": index_exists,
                "model": self.model,
                "similarity_top_k": self.similarity_top_k,
                "active_model": self.model_info,
                "using_openai": self.using_openai,
                "using_ollama": self.using_ollama
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
