"""
RAG Engine - Gestion de l'indexation et des requ√™tes
Utilise Llama-Index pour cr√©er et interroger un index vectoriel
"""
import os
from typing import List, Dict, Optional
from pathlib import Path

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, PromptTemplate, StorageContext, load_index_from_storage
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

# Essayer d'importer OpenAI, sinon utiliser Ollama
try:
    from llama_index.llms.openai import OpenAI
    from llama_index.embeddings.openai import OpenAIEmbedding
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

# Importer Ollama comme alternative
try:
    from llama_index.llms.ollama import Ollama
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False


class RAGEngine:
    """Moteur RAG pour SchoolReg"""
    
    def __init__(
        self,
        data_dir: str = "./data",
        storage_dir: str = "./storage",
        openai_api_key: Optional[str] = None,
        model: str = "gpt-4o-mini",
        temperature: float = 0.1,
        similarity_top_k: int = 5
    ):
        """
        Initialise le moteur RAG
        
        Args:
            data_dir: R√©pertoire contenant les documents sources
            storage_dir: R√©pertoire pour persister l'index
            openai_api_key: Cl√© API OpenAI
            model: Mod√®le OpenAI √† utiliser
            temperature: Temp√©rature pour la g√©n√©ration
            similarity_top_k: Nombre de passages √† r√©cup√©rer
        """
        self.data_dir = Path(data_dir)
        self.storage_dir = Path(storage_dir)
        self.model = model
        self.temperature = temperature
        self.similarity_top_k = similarity_top_k
        
        # Configuration OpenAI avec fallback automatique vers Ollama
        self.using_openai = False
        self.using_ollama = False
        self.model_info = "unknown"
        
        if openai_api_key and OPENAI_AVAILABLE:
            os.environ["OPENAI_API_KEY"] = openai_api_key
            
            # Essayer d'utiliser OpenAI pour le LLM UNIQUEMENT
            try:
                print("üîÑ Configuration d'OpenAI LLM (r√©ponses)...")
                Settings.llm = OpenAI(
                    model=self.model,
                    temperature=self.temperature,
                    max_tokens=2000
                )
                # TOUJOURS utiliser HuggingFace pour les embeddings (gratuit, local, pas de quota)
                print("üîÑ Configuration HuggingFace embeddings (recherche)...")
                Settings.embed_model = HuggingFaceEmbedding(
                    model_name="BAAI/bge-small-en-v1.5"
                )
                self.using_openai = True
                self.model_info = f"OpenAI {self.model} + HuggingFace embeddings"
                print(f"‚úÖ Mode hybride configur√©: {self.model_info}")
                print("üí° LLM: OpenAI (r√©ponses) | Embeddings: HuggingFace (recherche)")
            except Exception as e:
                print(f"‚ö†Ô∏è OpenAI non disponible: {e}")
                print("üîÑ Basculement automatique vers Ollama...")
                self.using_openai = False
        
        # Si OpenAI √©choue ou n'est pas disponible, utiliser Ollama
        if not self.using_openai:
            if OLLAMA_AVAILABLE:
                print("üîÑ Configuration d'Ollama (local & gratuit)...")
                Settings.llm = Ollama(
                    model="llama3.2",
                    temperature=self.temperature,
                    request_timeout=120.0
                )
                Settings.embed_model = HuggingFaceEmbedding(
                    model_name="BAAI/bge-small-en-v1.5"
                )
                self.using_ollama = True
                self.model_info = "Ollama llama3.2 (local)"
                print(f"‚úÖ Ollama configur√©: {self.model_info}")
                print("üí° Note: Assurez-vous qu'Ollama est lanc√© (ollama serve)")
            else:
                raise RuntimeError(
                    "Ni OpenAI ni Ollama ne sont disponibles. "
                    "Installez ollama ou configurez une cl√© OpenAI valide."
                )
        
        # Prompt personnalis√© en fran√ßais - Strict sur le contexte SchoolReg
        self.text_qa_template_str = (
            "Tu es un assistant virtuel pour SchoolReg, une plateforme de gestion scolaire. "
            "Tu dois √™tre professionnel, pr√©cis et utile.\n\n"
            
            "DOCUMENTATION DISPONIBLE:\n"
            "------------------------------\n"
            "{context_str}\n"
            "------------------------------\n\n"
            
            "R√àGLES DE R√âPONSE STRICTES:\n"
            "1. UNIQUEMENT si la question est une salutation initiale (bonjour, salut), r√©ponds chaleureusement et pr√©sente-toi BRI√àVEMENT.\n"
            "2. Pour TOUTES les autres questions, va DIRECTEMENT au contenu de la r√©ponse SANS salutation pr√©liminaire.\n"
            "3. Utilise UNIQUEMENT la documentation ci-dessus comme source d'information.\n"
            "4. Si la question ne concerne PAS SchoolReg (ex: politique, sport, cuisine, histoire, etc.), r√©ponds EXACTEMENT:\n"
            "   \"Je suis d√©sol√©, mais je suis sp√©cialis√© uniquement dans l'assistance sur SchoolReg. "
            "Je peux vous aider avec des questions sur la gestion des √©l√®ves, des classes, des paiements, des inscriptions, etc. "
            "Comment puis-je vous aider avec SchoolReg ? üìö\"\n"
            "5. Si l'info n'est pas dans la documentation mais concerne SchoolReg, explique ce que tu peux faire.\n"
            "6. Sois concis, clair et direct. Pas de formules de politesse r√©p√©titives.\n"
            "7. Utilise un √©moji occasionnel (1 max par r√©ponse) pour √™tre accueillant.\n"
            "8. R√©ponds TOUJOURS en fran√ßais.\n\n"
            
            "Question: {query_str}\n\n"
            "R√©ponse directe:"
        )
        self.text_qa_template = PromptTemplate(self.text_qa_template_str)
        
        # Charger ou cr√©er l'index
        self.index = None
        self.query_engine = None
        self._load_or_create_index()
    
    def _load_or_create_index(self):
        """Charge l'index existant ou en cr√©e un nouveau"""
        try:
            if self.storage_dir.exists() and any(self.storage_dir.iterdir()):
                print(f"üìÇ Chargement de l'index depuis {self.storage_dir}")
                storage_context = StorageContext.from_defaults(persist_dir=str(self.storage_dir))
                self.index = load_index_from_storage(storage_context)
                print("‚úÖ Index charg√© avec succ√®s")
            else:
                print(f"üî® Cr√©ation d'un nouvel index depuis {self.data_dir}")
                self._create_index()
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors du chargement de l'index: {e}")
            print("üî® Tentative de cr√©ation d'un nouvel index...")
            self._create_index()
        
        # Cr√©er le query engine
        if self.index:
            self.query_engine = self.index.as_query_engine(
                text_qa_template=self.text_qa_template,
                similarity_top_k=self.similarity_top_k
            )
    
    def _create_index(self):
        """Cr√©e un nouvel index √† partir des documents"""
        if not self.data_dir.exists():
            raise FileNotFoundError(f"Le dossier {self.data_dir} n'existe pas")
        
        # V√©rifier qu'il y a des documents
        files = list(self.data_dir.glob("*"))
        if not files:
            raise ValueError(f"Aucun document trouv√© dans {self.data_dir}")
        
        print(f"üìÑ Lecture de {len(files)} fichier(s)...")
        
        # Charger les documents
        documents = SimpleDirectoryReader(
            input_dir=str(self.data_dir),
            recursive=True
        ).load_data()
        
        print(f"üìä {len(documents)} document(s) charg√©(s)")
        
        # Cr√©er l'index avec HuggingFace embeddings (toujours)
        print("üî® Cr√©ation de l'index vectoriel...")
        self.index = VectorStoreIndex.from_documents(
            documents,
            show_progress=True
        )
        print("‚úÖ Index cr√©√© avec succ√®s")
        
        # Sauvegarder l'index
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        self.index.storage_context.persist(persist_dir=str(self.storage_dir))
        print(f"üíæ Index sauvegard√© dans {self.storage_dir}")
    
    def _is_off_topic(self, question: str) -> bool:
        """
        D√©tecte si une question est hors-sujet (ne concerne pas SchoolReg)
        
        Args:
            question: Question de l'utilisateur
            
        Returns:
            True si la question est hors-sujet, False sinon
        """
        question_lower = question.lower()
        
        # Mots-cl√©s autoris√©s li√©s √† SchoolReg
        schoolreg_keywords = [
            '√©l√®ve', 'eleve', '√©tudiant', 'student',
            'classe', 'cours', 'course',
            'paiement', 'payment', 'frais', 'tuition',
            'inscription', 'register', 'application',
            'parent', '√©cole', 'ecole', 'school',
            'professeur', 'enseignant', 'teacher',
            'note', 'grade', 'bulletin',
            'horaire', 'schedule',
            'admin', 'administrateur',
            'schoolreg', 'plateforme', 'syst√®me', 'systeme',
            'dashboard', 'tableau', 'bord',
            'connexion', 'login', 'compte', 'account',
            'bonjour', 'salut', 'hello', 'aide', 'help', 'comment'
        ]
        
        # Sujets clairement hors-sujet
        off_topic_keywords = [
            'recette', 'cuisine', 'cooking', 'recipe',
            'sport', 'football', 'basketball',
            'politique', 'president', 'election',
            'm√©t√©o', 'meteo', 'weather',
            'film', 'movie', 's√©rie', 'series',
            'musique', 'music', 'chanson',
            'voyage', 'travel', 'vacances',
            'voiture', 'car', 'auto',
            'sant√©', 'sante', 'health', 'm√©decin', 'medecin',
            'histoire', 'history', 'guerre',
            'science', 'chimie', 'physique', 'biologie',
            'math√©matique' if 'probl√®me' in question_lower or 'exercice' in question_lower else None
        ]
        off_topic_keywords = [k for k in off_topic_keywords if k]  # Enlever None
        
        # Si contient des mots hors-sujet √©vidents
        has_off_topic = any(keyword in question_lower for keyword in off_topic_keywords)
        
        # Si contient des mots SchoolReg OU si c'est une salutation courte
        has_schoolreg = any(keyword in question_lower for keyword in schoolreg_keywords)
        is_greeting = len(question.split()) <= 5 and any(g in question_lower for g in ['bonjour', 'salut', 'hello', 'hi'])
        
        # Hors-sujet si: contient des mots interdits ET ne contient pas de mots SchoolReg
        return has_off_topic and not has_schoolreg and not is_greeting
    
    def query(self, question: str) -> Dict:
        """
        Interroge l'index et g√©n√®re une r√©ponse
        
        Args:
            question: Question de l'utilisateur
            
        Returns:
            Dict contenant la r√©ponse et les sources
        """
        if not self.query_engine:
            raise RuntimeError("Le query engine n'est pas initialis√©")
        
        # V√©rifier si la question est hors-sujet
        if self._is_off_topic(question):
            return {
                "success": True,
                "question": question,
                "answer": (
                    "Je suis d√©sol√©, mais je suis sp√©cialis√© uniquement dans l'assistance sur SchoolReg. "
                    "Je peux vous aider avec des questions concernant :\n\n"
                    "üìö La gestion des √©l√®ves et inscriptions\n"
                    "üè´ Les classes et emplois du temps\n"
                    "üí∞ Les paiements et frais de scolarit√©\n"
                    "üë®‚Äçüë©‚Äçüëß Les comptes parents et √©l√®ves\n"
                    "üìä Le tableau de bord administratif\n\n"
                    "Comment puis-je vous aider avec SchoolReg aujourd'hui ?"
                ),
                "sources": [],
                "model_used": "Off-topic filter",
                "using_openai": False,
                "using_ollama": False,
                "off_topic": True
            }
        
        try:
            # Ex√©cuter la requ√™te
            response = self.query_engine.query(question)
            
            # Extraire les sources
            sources = []
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes:
                    metadata = node.node.metadata if hasattr(node.node, 'metadata') else {}
                    sources.append({
                        "file_name": metadata.get('file_name', 'Unknown'),
                        "page": metadata.get('page_label'),
                        "score": float(node.score) if hasattr(node, 'score') else None,
                        "text_preview": node.text[:200] + "..." if len(node.text) > 200 else node.text
                    })
            
            return {
                "success": True,
                "question": question,
                "answer": str(response),
                "sources": sources,
                "model_used": self.model_info,
                "using_openai": self.using_openai,
                "using_ollama": self.using_ollama
            }
        
        except Exception as e:
            return {
                "success": False,
                "question": question,
                "error": str(e),
                "model_used": self.model_info
            }
    
    def search(self, question: str) -> Dict:
        """
        Recherche les passages pertinents sans g√©n√©ration
        
        Args:
            question: Question de l'utilisateur
            
        Returns:
            Dict contenant les passages pertinents
        """
        if not self.index:
            raise RuntimeError("L'index n'est pas initialis√©")
        
        try:
            # Cr√©er un retriever
            retriever = self.index.as_retriever(
                similarity_top_k=self.similarity_top_k
            )
            
            # R√©cup√©rer les n≈ìuds pertinents
            nodes = retriever.retrieve(question)
            
            # Formater les r√©sultats
            results = []
            for node in nodes:
                metadata = node.node.metadata if hasattr(node.node, 'metadata') else {}
                results.append({
                    "file_name": metadata.get('file_name', 'Unknown'),
                    "page": metadata.get('page_label'),
                    "score": float(node.score) if hasattr(node, 'score') else None,
                    "text": node.text
                })
            
            return {
                "success": True,
                "question": question,
                "results": results,
                "count": len(results)
            }
        
        except Exception as e:
            # Fallback runtime: g√©rer erreurs embeddings OpenAI et LLM OpenAI
            print(f"‚ö†Ô∏è Erreur durant la requ√™te: {e}")
            err = str(e).lower()
            try_again = False
            # 1) Probl√®me embeddings OpenAI (403 / model_not_found)
            if self.using_openai and ("model_not_found" in err or "does not have access to model" in err or "embedding" in err):
                try:
                    print("üîÑ Basculement embeddings ‚ûú HuggingFace et reconstruction index...")
                    from llama_index.embeddings.huggingface import HuggingFaceEmbedding as _HF
                    Settings.embed_model = _HF(model_name="BAAI/bge-small-en-v1.5")
                    self.model_info = (self.model_info + " + HF embeddings").strip()
                    # Recr√©er l'index et le query engine
                    self.refresh_index()
                    try_again = True
                except Exception as e_emb:
                    print(f"‚ùå √âchec fallback embeddings: {e_emb}")
            # 2) Probl√®me LLM OpenAI (quota / rate limit / indisponible)
            if (self.using_openai and ("openai" in err or "quota" in err or "rate limit" in err)) and OLLAMA_AVAILABLE:
                print("üîÑ Basculement LLM ‚ûú Ollama et nouvelle tentative...")
                from llama_index.llms.ollama import Ollama as _O
                Settings.llm = _O(model="llama3.2", temperature=self.temperature, request_timeout=120.0)
                self.using_openai = False
                self.using_ollama = True
                self.model_info = "Ollama llama3.2 (fallback)"
                try_again = True
            if try_again:
                try:
                    response = self.query_engine.query(question)
                    sources = []
                    if hasattr(response, 'source_nodes'):
                        for node in response.source_nodes:
                            metadata = node.node.metadata if hasattr(node.node, 'metadata') else {}
                            sources.append({
                                "file_name": metadata.get('file_name', 'Unknown'),
                                "page": metadata.get('page_label'),
                                "score": float(node.score) if hasattr(node, 'score') else None,
                                "text_preview": node.text[:200] + "..." if len(node.text) > 200 else node.text
                            })
                    return {
                        "success": True,
                        "question": question,
                        "answer": str(response),
                        "sources": sources,
                        "model_used": self.model_info,
                        "using_openai": self.using_openai,
                        "using_ollama": self.using_ollama
                    }
                except Exception as e2:
                    print(f"‚ùå Nouvelle tentative √©chou√©e: {e2}")
                    return {
                        "success": False,
                        "question": question,
                        "error": str(e2),
                        "model_used": self.model_info
                    }
            return {
                "success": False,
                "question": question,
                "error": str(e),
                "model_used": self.model_info
            }
    
    def refresh_index(self):
        """Reconstruit l'index √† partir des documents"""
        print("üîÑ Reconstruction de l'index...")
        
        # Supprimer l'ancien index
        if self.storage_dir.exists():
            import shutil
            shutil.rmtree(self.storage_dir)
        
        # Recr√©er l'index
        self._create_index()
        
        # Recr√©er le query engine
        if self.index:
            self.query_engine = self.index.as_query_engine(
                text_qa_template=self.text_qa_template,
                similarity_top_k=self.similarity_top_k
            )
        
        print("‚úÖ Index reconstruit avec succ√®s")
    
    def get_stats(self) -> Dict:
        """Retourne des statistiques sur l'index"""
        try:
            doc_count = len(list(self.data_dir.glob("*"))) if self.data_dir.exists() else 0
            index_exists = self.storage_dir.exists() and any(self.storage_dir.iterdir())
            
            return {
                "success": True,
                "data_dir": str(self.data_dir),
                "storage_dir": str(self.storage_dir),
                "documents_count": doc_count,
                "index_exists": index_exists,
                "model": self.model,
                "similarity_top_k": self.similarity_top_k,
                "active_model": self.model_info,
                "using_openai": self.using_openai,
                "using_ollama": self.using_ollama
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
